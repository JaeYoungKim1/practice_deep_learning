from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras import optimizers
import tensorflow as tf
#layer = layers.Dense(32, input_shape=(784,))  ## 첫번째 차원이 784인 벡터 텐서 만 입력받는 밀집층 생성, 차원크기가 32로 변환된 텐서를 출력
#a = tf.constant([1,2,3])
#print(a.shape)
#model = models.Sequential() # 모델 생성
#model.add(layers.Dense(32,input_shape=(784,))) #모델에 784개의 벡터텐서를 받아 32개 벡터로 출력하는 레이어 추가
#model.add(layers.Dense(10)) # 모델에 32개 벡터(지정안해도 상위 레이어 참고해서 받음)텐서를 받아 10개 벡터로 출력하는 레이어 추가

## --- 여기까지가 레이어(층) 설명 ---, 층에는 하나 이상의 텐서 인풋, 하나 이상의 텐서 아웃풋 데이터 처리 모듈임! 대부분 층은 가중치를 가지고, 가중치는 SGD에 의해 학습되는 텐서로, 이 값을 기반으로 아웃풋 예측.

## 모델 = 층의 네트워크로 비순환 유향그래프이다. 네트워크 구조는 가설공간(가능성있는 공간)을 정의한다. 가설공간을 인풋에서 아웃풋으로 매핑하는 텐서 연산으로 제한한다. 우리가 찾아야 할 것은 텐서 연산에 포함된 가중치 텐서의 좋은값!

## 손실함수와 옵티마이저, 네트워크 구조를 정의하고 나서 손실함수와 옵티마이저를 선택해야 한다. 손실함수 : 훈련 동안 최소화할 값이며 주어진 문제의 성공지표가 된다. 옵티마이저 : 손실함수를 기반으로 네트워크가 어떻게 업데이트 될지 결정하며, 특정 종류의 SGD를 구현한다.
## 아웃풋을 여러개 내는 신경망은 손실함수도 여러개 가질 수 있으나, 그래디언트 구하는 과정에서 쓰일 loss score는 하나의 값이어야 하므로, 모든 손실을 평균을 내서 하나의 스칼라 양으로 합쳐진다.
## 2개 클래스 분류문제 : binary crossentropy , 여러개 클래스 분류 문제 : categorical crossentropy, 회귀문제 : mean-square error , 시퀀스 학습 문제 : CTC 등을 사용 , 그 외엔 독자적인 목적함수를 만듦

## 이에 관련한 예제 실습을 위해 케라스 라이브러리를 사용
## 전형적인 케라스 작업 흐름 1. 입력텐서와 타깃 텐서로 이루어진 훈련데이터 정의 2. 입력과 타깃을 매핑하는 층으로 이루어진 네트워크 or 모델 정의 3. 손실함수, 옵티마이저, 모니터링을 위한 측정지표 선택하여 학습과정 설정
## 4. 훈련데이터에 대해 모델의 fit() 메서드를 반복적으로 호출
## 2번부터 보면, 모델을 정의할 땐 Sequential 클래스를 사용하거나 함수형 API를 사용하는 방법이 있습니다. 시퀀셜 클래스는 층을 순서대로 쌓아올린 네트워크이고, 함수형 API는 완전히 임의의 구조를 만들수있는 비순환유향그래프를 만든다. 시퀀셜은 직관적이고 편리하나, 단순히 층을 쌓는것 만으로는 복잡한 인공신경망을 구현할 수 없다.
# model = models.Sequential()
# model.add(layers.Dense(32,activation='relu',input_shape=(784,)))
# model.add(layers.Dense(10,activation='softmax')) # 시퀀셜 클래스 사용해서 2개의 층으로 모델을 정의
input_tensor = layers.Input(shape=(784,))
x = layers.Dense(32,activation='relu')(input_tensor)
output_tensor = layers.Dense(10, activation='softmax')(x)
model = models.Model(inputs = input_tensor, outputs = output_tensor) #함수형 API를 사용하여 모델이 처리할 데이터 텐서를 만들고 마치 함수처럼 이 텐서에 층을 적용
#모델 구조가 정의된 후 단계는 컴파일단계로, 학습과정이 설정된다. 옵티마이저와 손실함수, 훈련중 모니터링하기 위해 필요한 측정지표를 지정한다.
model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='mse',metrics=['accuracy'])
#마지막으로 입력데이터의 넘파이 배열, 상응하는 타겟데이터를 모델의 fit()메서드에 전달함으로써 학습과정이 이루어진다.
#model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) # 입력데이터에 해당하는 정답 - 타겟텐서가 있으면 됨

# 영화 리뷰 분류 - 긍정 텍스트, 부정 텍스트
# 데이터셋은 IMDB에서 가져온 양극단의 리뷰 5만개 데이터셋 훈련데이터 절반, 테스트 데이터 절반 긍정 부정 절반으로 구성
# 데이터는 전처리되어 있어 각 리뷰가 숫자 시퀀스로 변환되어 있음.
from tensorflow.keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) # train data에서 가장 자주 나타나는 단어 1만개 사용하겠다는 의미
print(train_data[0])
print(train_labels[0])

# 데이터 준비, 숫자 리스트들을 텐서로 바꿔서 신경망에 주입해야한다. 텐서로 바꾸는 두가지 방법은 1. 같은 길이가 되도록 리스트에 패딩을 추가하고(samples, sequence_length)크기의 정수 텐서로 변환하여 이 정수텐서를 다룰 수 있는 층을 신경망의 첫번째 층으로 사용한다.(임베딩 층)
# 2. 리스트를 원-핫 인코딩하여 0과 1의 벡터로 변환한다. [3,5]인 경우엔 3과 5를 1로, 그 외 0으로 이루어진 1만차원의 벡터로 변환한다. 그다음 부동 소수 벡터 데이터를 다룰 수 있는 Dense층을 신경망의 첫번째 층으로 사용한다.
#두번째 방법 정수 시퀀스를 이진 행렬로 인코딩하기
import numpy as np
def vectorize_sequence(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension)) # 크기가 이렇게 되는 모든 원소가 0인 행렬을 만든다.
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1. # results[i]에서 특정 인덱스의 위치를 1로 만든다.
    return results
x_train = vectorize_sequence(train_data)
x_test = vectorize_sequence(test_data) # 데이터를 벡터로 변환
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32') # 라벨을 데이터 타입에 맞게 변환

# 신경망 모델 만들기
# 입력 데이터가 벡터고 라벨은 스칼라이다. 해당 문제에 잘 작동하는 네트워크 종류는 relu 활성화 함수를 사용한 fully connected dense( Dense(16, activation = 'relu') )를 쌓은것이 된다.
# Dense 층에 전달한 매개변수 16은 hidden unit의 수이다.

# 추가로, 인풋을 넣을때 연산이 어떻게 되는 지, activation function은 언제 쓰는 지, 옵티마이저 흐름도 설명- SGD 이후 모멘텀 등장배경과 그 기법들 등장배경과 기법별 수식추가, 그림 추가

#신경망의 구조에 대한 발표 - 층, 층이 모인 네트워크, 손실함수와 옵티마이저로 학습과정 조절 하는 구조고, 이에 관련한 실습예제 케라스 소개하고, 3가지 예측문제 소개하고, 보충자료 추가